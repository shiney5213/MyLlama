{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, LlamaTokenizer\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@vyperius117/understanding-the-llama2-tokenizer-working-with-the-tokenizer-locally-using-transformers-2e0f9e69d786\n",
    "- tokenizer.model → 실제 토크나이저 모델 파일\n",
    "- tokenizer.json → 토크나이저에 대한 어휘 구성\n",
    "- okenizer_config.json → 토크나이저 구성 파일\n",
    "\n",
    "\n",
    "# llama2 : 문장 중심 BPE 방법 사용\n",
    "- https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root = '../llama_models/llama-2-7b-hf'\n",
    "tokenizer_model_path = os.path.join(root, 'tokenizer.model')\n",
    "tokenizer_json_path = os.path.join(root,'tokenizer.json')\n",
    "tokenizer_config_path =os.path.join(root, 'tokenizer_config.json')\n",
    "tokenizer_checkpoint_path = os.path.join(root, '')\n",
    "\n",
    "print(os.path.isfile(tokenizer_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<property at 0x2833a9ad810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'tokenizer.model'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.vocab_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_added_tokens_decoder',\n",
       " '_added_tokens_encoder',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_batch_prepare_for_model',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_compile_jinja_template',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenize',\n",
       " '_unk_token',\n",
       " '_update_trie',\n",
       " '_upload_modified_files',\n",
       " 'add_bos_token',\n",
       " 'add_eos_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'default_chat_template',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_spm_processor',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'legacy',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_for_tokenization',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'slow_tokenizer_class',\n",
       " 'sp_model',\n",
       " 'sp_model_kwargs',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'tokens_trie',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'unk_token_length',\n",
       " 'use_default_system_prompt',\n",
       " 'verbose',\n",
       " 'vocab_file',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../llama_models/llama-2-7b-hf\\\\tokenizer.model'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. tokenizer.json 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_json_path)\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(tokenizer_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_compile_jinja_template',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'default_chat_template',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fast_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(fast_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fast_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "builtin_function_or_method"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fast_tokenizer.vocab.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fast_tokenizer.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = fast_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수 30970\n",
      "무 31716\n",
      "소 31189\n",
      "연 31285\n",
      "만 31826\n",
      "교 31972\n",
      "드 31493\n",
      "민 31582\n",
      "차 31817\n",
      "전 31170\n",
      "재 31973\n",
      "에 31054\n",
      "정 30852\n",
      "원 31198\n",
      "안 31734\n",
      "식 31895\n",
      "구 31231\n",
      "면 31747\n",
      "해 31435\n",
      "로 30906\n",
      "문 31406\n",
      "경 31378\n",
      "한 30877\n",
      "오 31346\n",
      "화 31225\n",
      "미 31362\n",
      "일 31153\n",
      "요 31527\n",
      "회 31953\n",
      "진 31536\n",
      "자 31013\n",
      "인 30918\n",
      "과 31906\n",
      "위 31724\n",
      "라 31197\n",
      "강 31774\n",
      "이 30393\n",
      "을 31286\n",
      "씨 31781\n",
      "호 31603\n",
      "유 31533\n",
      "개 31789\n",
      "천 31563\n",
      "니 31063\n",
      "비 31487\n",
      "사 30791\n",
      "년 31571\n",
      "보 31199\n",
      "산 31458\n",
      "합 31980\n",
      "용 31737\n",
      "명 31976\n",
      "선 31345\n",
      "박 31736\n",
      "방 31945\n",
      "현 31680\n",
      "다 30709\n",
      "김 31102\n",
      "왕 31996\n",
      "상 31158\n",
      "모 31962\n",
      "지 30811\n",
      "터 31856\n",
      "시 30889\n",
      "서 31093\n",
      "남 31754\n",
      "세 31578\n",
      "제 31306\n",
      "동 31000\n",
      "는 31081\n",
      "도 31136\n",
      "우 31327\n",
      "들 31804\n",
      "조 31408\n",
      "어 31129\n",
      "단 31746\n",
      "역 31987\n",
      "스 30784\n",
      "를 31517\n",
      "군 31699\n",
      "신 31262\n",
      "월 31950\n",
      "국 31293\n",
      "아 30860\n",
      "타 31925\n",
      "여 31457\n",
      "부 31279\n",
      "마 31417\n",
      "성 31126\n",
      "리 30826\n",
      "기 30827\n",
      "의 30708\n",
      "대 30890\n",
      "음 31966\n",
      "주 30981\n",
      "가 30903\n",
      "은 31354\n",
      "내 31940\n",
      "나 31207\n",
      "바 31963\n",
      "하 30944\n",
      "학 31822\n",
      "트 31177\n",
      "그 31607\n",
      "종 31930\n",
      "백 31989\n",
      "영 31288\n",
      "장 31299\n",
      "고 31137\n",
      "공 31334\n",
      "중 31941\n"
     ]
    }
   ],
   "source": [
    "korean_reg = re.compile(r'[ㄱ-ㅣ가-힣]')\n",
    "korean_dict = {}\n",
    "for key, value in vocab_dict.items():\n",
    "    if korean_reg.match(key):\n",
    "        print(key, value)\n",
    "        korean_dict[key] = value\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(korean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['수', '무', '소', '연', '만', '교', '드', '민', '차', '전', '재', '에', '정', '원', '안', '식', '구', '면', '해', '로', '문', '경', '한', '오', '화', '미', '일', '요', '회', '진', '자', '인', '과', '위', '라', '강', '이', '을', '씨', '호', '유', '개', '천', '니', '비', '사', '년', '보', '산', '합', '용', '명', '선', '박', '방', '현', '다', '김', '왕', '상', '모', '지', '터', '시', '서', '남', '세', '제', '동', '는', '도', '우', '들', '조', '어', '단', '역', '스', '를', '군', '신', '월', '국', '아', '타', '여', '부', '마', '성', '리', '기', '의', '대', '음', '주', '가', '은', '내', '나', '바', '하', '학', '트', '그', '종', '백', '영', '장', '고', '공', '중'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31822"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['학']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['osci',\n",
       " 'ammer',\n",
       " 'прави',\n",
       " '▁happening',\n",
       " 'Ă',\n",
       " '▁specifying',\n",
       " 'itzerland',\n",
       " '▁crow',\n",
       " 'inas',\n",
       " '{.',\n",
       " '▁Националь',\n",
       " '▁select',\n",
       " 'bounds',\n",
       " '▁cuer',\n",
       " 'тан',\n",
       " 'sdl',\n",
       " '龙',\n",
       " 'adding',\n",
       " '▁Dav',\n",
       " '▁lavor',\n",
       " 'htm',\n",
       " '▁tons',\n",
       " '▁Fr',\n",
       " \"`'\",\n",
       " 'ject',\n",
       " '▁começ',\n",
       " '면',\n",
       " 'preventDefault',\n",
       " 'undefined',\n",
       " '▁fu',\n",
       " 'lywood',\n",
       " 'た',\n",
       " '▁dével',\n",
       " 'дова',\n",
       " 'yan',\n",
       " '▁once',\n",
       " '▁pak',\n",
       " 'isk',\n",
       " '▁gender',\n",
       " 'ப',\n",
       " '▁intellig',\n",
       " 'Đ',\n",
       " 'дав',\n",
       " 'tis',\n",
       " 'сона',\n",
       " '▁atmos',\n",
       " '}},',\n",
       " 'erei',\n",
       " 'inflate',\n",
       " 'dw',\n",
       " '▁-->',\n",
       " '▁Philadelphia',\n",
       " 'NET',\n",
       " '▁answering',\n",
       " 'giv',\n",
       " 'rewrite',\n",
       " 'ALSE',\n",
       " '▁usage',\n",
       " 'gas',\n",
       " '▁p',\n",
       " 'REE',\n",
       " '▁curiosity',\n",
       " 'idos',\n",
       " 'acing',\n",
       " 'IMAGE',\n",
       " '▁Lot',\n",
       " '▁Deutsch',\n",
       " 'MODE',\n",
       " 'href',\n",
       " 'elijke',\n",
       " '值',\n",
       " 'nak',\n",
       " 'Why',\n",
       " 'Selection',\n",
       " 'PAR',\n",
       " 'amar',\n",
       " '▁Georgia',\n",
       " '<0x13>',\n",
       " '▁Guardian',\n",
       " '▁free',\n",
       " 'labels',\n",
       " 'Len',\n",
       " '▁outras',\n",
       " '▁relate',\n",
       " '▁Ig',\n",
       " 'kon',\n",
       " 'reate',\n",
       " 'fahr',\n",
       " 'ædia',\n",
       " 'alf',\n",
       " 'asha',\n",
       " 'ach',\n",
       " '▁foreach',\n",
       " '▁Sand',\n",
       " 'Bind',\n",
       " '<0x4E>',\n",
       " '▁І',\n",
       " '▁Publish',\n",
       " 'FileName',\n",
       " 'дні',\n",
       " 'пер',\n",
       " '▁conj',\n",
       " '▁looks',\n",
       " 'bourne',\n",
       " '▁march',\n",
       " '▁satisfied',\n",
       " '明',\n",
       " '▁Sy',\n",
       " 'Variable',\n",
       " 'umen',\n",
       " '▁particulier',\n",
       " '▁chairman',\n",
       " 'limp',\n",
       " 'ся',\n",
       " 'alph',\n",
       " '▁получил',\n",
       " '▁fait',\n",
       " 'oso',\n",
       " '▁fixing',\n",
       " '▁fashion']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_dict.keys())[:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BertWordPieceTokenizer : BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)\n",
    "- CharBPETokenizer : 오리지널 BPE\n",
    "- ByteLevelBPETokenizer : BPE의 바이트 레벨 버전\n",
    "- SentencePieceBPETokenizer : 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama-2-ko-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../llama_models/llama-2-ko-7b'\n",
    "json_path = os.path.join(root, 'tokenizer.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46336\n"
     ]
    }
   ],
   "source": [
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=json_path)\n",
    "\n",
    "print(len(fast_tokenizer.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_reg2 = re.compile(r'[ㄱ-ㅣ가-힣]')\n",
    "\n",
    "korean_dict2 = {}\n",
    "\n",
    "for key, value in fast_tokenizer.vocab.items():\n",
    "    if korean_reg2.match(key):\n",
    "        korean_dict2[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['지긋', '싸고', '니까', '가야', '받지', '롯데', '팀', '불어', '간담', '틸', '총', '둔', '전쟁', '대지', '시장에', '주를', '과제', '잔아', '추진', '이노', '활', '랫폼', '났네', '고에', '보며', '다는게', '려요', '육', '정이', '주냐', '폐', '흙', '싶다', '슷', '뺑', '해야지', '네요', '룬', '는구나', '흥민', '지막', '입', '췌', '였네', '려라', '국민을', '통에', '하기는', '본부는', '적', '햄', '왜구', '뎅', '하시는', '금으로', '거야', '겠네', '이에', '유산', '갠', '내면', '것처럼', '아아', '트를', '렸습니다', '으니', '감으로', '회사', '호텔', '꼬', '핏', '핌', '경은', '국과', '악', '지를', '형을', '박물', '심에서', '상납', '안전', '챗', '깥', '세가', '처분', '롯', '절', '제도', '되니', '누구', '인민', '들로', '마음에', '려니', '뀔', '할것', '트롤', '입국', '진보', '습']\n"
     ]
    }
   ],
   "source": [
    "print(list(korean_dict2.keys())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5527\n"
     ]
    }
   ],
   "source": [
    "print(len(korean_dict2.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('llama_recipes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "370669f31d56c793d418c7345ca8413592cca298019e995be7fff47287a2522c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
