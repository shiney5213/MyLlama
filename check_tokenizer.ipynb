{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, LlamaTokenizer\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@vyperius117/understanding-the-llama2-tokenizer-working-with-the-tokenizer-locally-using-transformers-2e0f9e69d786\n",
    "- tokenizer.model → 실제 토크나이저 모델 파일\n",
    "- tokenizer.json → 토크나이저에 대한 어휘 구성\n",
    "- okenizer_config.json → 토크나이저 구성 파일\n",
    "\n",
    "\n",
    "# llama2 : 문장 중심 BPE 방법 사용\n",
    "- https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root = '../llama_models/llama-2-7b-hf'\n",
    "tokenizer_model_path = os.path.join(root, 'tokenizer.model')\n",
    "tokenizer_json_path = os.path.join(root,'tokenizer.json')\n",
    "tokenizer_config_path =os.path.join(root, 'tokenizer_config.json')\n",
    "tokenizer_checkpoint_path = os.path.join(root, '')\n",
    "\n",
    "print(os.path.isfile(tokenizer_model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<property at 0x2833a9ad810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'tokenizer.model'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.vocab_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LlamaTokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'bos_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSPECIAL_TOKENS_ATTRIBUTES\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'bos_token'"
     ]
    }
   ],
   "source": [
    "LlamaTokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPECIAL_TOKENS_ATTRIBUTES', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_tokens', '_added_tokens_decoder', '_added_tokens_encoder', '_additional_special_tokens', '_auto_class', '_batch_encode_plus', '_batch_prepare_for_model', '_bos_token', '_call_one', '_cls_token', '_compile_jinja_template', '_convert_id_to_token', '_convert_token_to_id', '_convert_token_to_id_with_added_voc', '_create_repo', '_decode', '_decode_use_source_tokenizer', '_encode_plus', '_eos_token', '_eventual_warn_about_too_long_sequence', '_eventually_correct_t5_max_length', '_from_pretrained', '_get_files_timestamps', '_get_padding_truncation_strategies', '_in_target_context_manager', '_mask_token', '_pad', '_pad_token', '_pad_token_type_id', '_processor_class', '_save_pretrained', '_sep_token', '_set_processor_class', '_switch_to_input_mode', '_switch_to_target_mode', '_tokenize', '_unk_token', '_update_trie', '_upload_modified_files', 'add_bos_token', 'add_eos_token', 'add_special_tokens', 'add_tokens', 'added_tokens_decoder', 'added_tokens_encoder', 'additional_special_tokens', 'additional_special_tokens_ids', 'all_special_ids', 'all_special_tokens', 'all_special_tokens_extended', 'apply_chat_template', 'as_target_tokenizer', 'batch_decode', 'batch_encode_plus', 'bos_token', 'bos_token_id', 'build_inputs_with_special_tokens', 'chat_template', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id', 'convert_added_tokens', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'default_chat_template', 'deprecation_warnings', 'encode', 'encode_plus', 'eos_token', 'eos_token_id', 'from_pretrained', 'get_added_vocab', 'get_special_tokens_mask', 'get_spm_processor', 'get_vocab', 'init_inputs', 'init_kwargs', 'is_fast', 'legacy', 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'max_model_input_sizes', 'model_input_names', 'model_max_length', 'name_or_path', 'num_special_tokens_to_add', 'pad', 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_for_tokenization', 'prepare_seq2seq_batch', 'pretrained_init_configuration', 'pretrained_vocab_files_map', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'sep_token', 'sep_token_id', 'slow_tokenizer_class', 'sp_model', 'sp_model_kwargs', 'special_tokens_map', 'special_tokens_map_extended', 'split_special_tokens', 'tokenize', 'tokens_trie', 'truncate_sequences', 'truncation_side', 'unk_token', 'unk_token_id', 'unk_token_length', 'use_default_system_prompt', 'verbose', 'vocab_file', 'vocab_files_names', 'vocab_size']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../llama_models/llama-2-7b-hf\\\\tokenizer.model'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " '<s>': 1,\n",
       " '</s>': 2,\n",
       " '<0x00>': 3,\n",
       " '<0x01>': 4,\n",
       " '<0x02>': 5,\n",
       " '<0x03>': 6,\n",
       " '<0x04>': 7,\n",
       " '<0x05>': 8,\n",
       " '<0x06>': 9,\n",
       " '<0x07>': 10,\n",
       " '<0x08>': 11,\n",
       " '<0x09>': 12,\n",
       " '<0x0A>': 13,\n",
       " '<0x0B>': 14,\n",
       " '<0x0C>': 15,\n",
       " '<0x0D>': 16,\n",
       " '<0x0E>': 17,\n",
       " '<0x0F>': 18,\n",
       " '<0x10>': 19,\n",
       " '<0x11>': 20,\n",
       " '<0x12>': 21,\n",
       " '<0x13>': 22,\n",
       " '<0x14>': 23,\n",
       " '<0x15>': 24,\n",
       " '<0x16>': 25,\n",
       " '<0x17>': 26,\n",
       " '<0x18>': 27,\n",
       " '<0x19>': 28,\n",
       " '<0x1A>': 29,\n",
       " '<0x1B>': 30,\n",
       " '<0x1C>': 31,\n",
       " '<0x1D>': 32,\n",
       " '<0x1E>': 33,\n",
       " '<0x1F>': 34,\n",
       " '<0x20>': 35,\n",
       " '<0x21>': 36,\n",
       " '<0x22>': 37,\n",
       " '<0x23>': 38,\n",
       " '<0x24>': 39,\n",
       " '<0x25>': 40,\n",
       " '<0x26>': 41,\n",
       " '<0x27>': 42,\n",
       " '<0x28>': 43,\n",
       " '<0x29>': 44,\n",
       " '<0x2A>': 45,\n",
       " '<0x2B>': 46,\n",
       " '<0x2C>': 47,\n",
       " '<0x2D>': 48,\n",
       " '<0x2E>': 49,\n",
       " '<0x2F>': 50,\n",
       " '<0x30>': 51,\n",
       " '<0x31>': 52,\n",
       " '<0x32>': 53,\n",
       " '<0x33>': 54,\n",
       " '<0x34>': 55,\n",
       " '<0x35>': 56,\n",
       " '<0x36>': 57,\n",
       " '<0x37>': 58,\n",
       " '<0x38>': 59,\n",
       " '<0x39>': 60,\n",
       " '<0x3A>': 61,\n",
       " '<0x3B>': 62,\n",
       " '<0x3C>': 63,\n",
       " '<0x3D>': 64,\n",
       " '<0x3E>': 65,\n",
       " '<0x3F>': 66,\n",
       " '<0x40>': 67,\n",
       " '<0x41>': 68,\n",
       " '<0x42>': 69,\n",
       " '<0x43>': 70,\n",
       " '<0x44>': 71,\n",
       " '<0x45>': 72,\n",
       " '<0x46>': 73,\n",
       " '<0x47>': 74,\n",
       " '<0x48>': 75,\n",
       " '<0x49>': 76,\n",
       " '<0x4A>': 77,\n",
       " '<0x4B>': 78,\n",
       " '<0x4C>': 79,\n",
       " '<0x4D>': 80,\n",
       " '<0x4E>': 81,\n",
       " '<0x4F>': 82,\n",
       " '<0x50>': 83,\n",
       " '<0x51>': 84,\n",
       " '<0x52>': 85,\n",
       " '<0x53>': 86,\n",
       " '<0x54>': 87,\n",
       " '<0x55>': 88,\n",
       " '<0x56>': 89,\n",
       " '<0x57>': 90,\n",
       " '<0x58>': 91,\n",
       " '<0x59>': 92,\n",
       " '<0x5A>': 93,\n",
       " '<0x5B>': 94,\n",
       " '<0x5C>': 95,\n",
       " '<0x5D>': 96,\n",
       " '<0x5E>': 97,\n",
       " '<0x5F>': 98,\n",
       " '<0x60>': 99,\n",
       " '<0x61>': 100,\n",
       " '<0x62>': 101,\n",
       " '<0x63>': 102,\n",
       " '<0x64>': 103,\n",
       " '<0x65>': 104,\n",
       " '<0x66>': 105,\n",
       " '<0x67>': 106,\n",
       " '<0x68>': 107,\n",
       " '<0x69>': 108,\n",
       " '<0x6A>': 109,\n",
       " '<0x6B>': 110,\n",
       " '<0x6C>': 111,\n",
       " '<0x6D>': 112,\n",
       " '<0x6E>': 113,\n",
       " '<0x6F>': 114,\n",
       " '<0x70>': 115,\n",
       " '<0x71>': 116,\n",
       " '<0x72>': 117,\n",
       " '<0x73>': 118,\n",
       " '<0x74>': 119,\n",
       " '<0x75>': 120,\n",
       " '<0x76>': 121,\n",
       " '<0x77>': 122,\n",
       " '<0x78>': 123,\n",
       " '<0x79>': 124,\n",
       " '<0x7A>': 125,\n",
       " '<0x7B>': 126,\n",
       " '<0x7C>': 127,\n",
       " '<0x7D>': 128,\n",
       " '<0x7E>': 129,\n",
       " '<0x7F>': 130,\n",
       " '<0x80>': 131,\n",
       " '<0x81>': 132,\n",
       " '<0x82>': 133,\n",
       " '<0x83>': 134,\n",
       " '<0x84>': 135,\n",
       " '<0x85>': 136,\n",
       " '<0x86>': 137,\n",
       " '<0x87>': 138,\n",
       " '<0x88>': 139,\n",
       " '<0x89>': 140,\n",
       " '<0x8A>': 141,\n",
       " '<0x8B>': 142,\n",
       " '<0x8C>': 143,\n",
       " '<0x8D>': 144,\n",
       " '<0x8E>': 145,\n",
       " '<0x8F>': 146,\n",
       " '<0x90>': 147,\n",
       " '<0x91>': 148,\n",
       " '<0x92>': 149,\n",
       " '<0x93>': 150,\n",
       " '<0x94>': 151,\n",
       " '<0x95>': 152,\n",
       " '<0x96>': 153,\n",
       " '<0x97>': 154,\n",
       " '<0x98>': 155,\n",
       " '<0x99>': 156,\n",
       " '<0x9A>': 157,\n",
       " '<0x9B>': 158,\n",
       " '<0x9C>': 159,\n",
       " '<0x9D>': 160,\n",
       " '<0x9E>': 161,\n",
       " '<0x9F>': 162,\n",
       " '<0xA0>': 163,\n",
       " '<0xA1>': 164,\n",
       " '<0xA2>': 165,\n",
       " '<0xA3>': 166,\n",
       " '<0xA4>': 167,\n",
       " '<0xA5>': 168,\n",
       " '<0xA6>': 169,\n",
       " '<0xA7>': 170,\n",
       " '<0xA8>': 171,\n",
       " '<0xA9>': 172,\n",
       " '<0xAA>': 173,\n",
       " '<0xAB>': 174,\n",
       " '<0xAC>': 175,\n",
       " '<0xAD>': 176,\n",
       " '<0xAE>': 177,\n",
       " '<0xAF>': 178,\n",
       " '<0xB0>': 179,\n",
       " '<0xB1>': 180,\n",
       " '<0xB2>': 181,\n",
       " '<0xB3>': 182,\n",
       " '<0xB4>': 183,\n",
       " '<0xB5>': 184,\n",
       " '<0xB6>': 185,\n",
       " '<0xB7>': 186,\n",
       " '<0xB8>': 187,\n",
       " '<0xB9>': 188,\n",
       " '<0xBA>': 189,\n",
       " '<0xBB>': 190,\n",
       " '<0xBC>': 191,\n",
       " '<0xBD>': 192,\n",
       " '<0xBE>': 193,\n",
       " '<0xBF>': 194,\n",
       " '<0xC0>': 195,\n",
       " '<0xC1>': 196,\n",
       " '<0xC2>': 197,\n",
       " '<0xC3>': 198,\n",
       " '<0xC4>': 199,\n",
       " '<0xC5>': 200,\n",
       " '<0xC6>': 201,\n",
       " '<0xC7>': 202,\n",
       " '<0xC8>': 203,\n",
       " '<0xC9>': 204,\n",
       " '<0xCA>': 205,\n",
       " '<0xCB>': 206,\n",
       " '<0xCC>': 207,\n",
       " '<0xCD>': 208,\n",
       " '<0xCE>': 209,\n",
       " '<0xCF>': 210,\n",
       " '<0xD0>': 211,\n",
       " '<0xD1>': 212,\n",
       " '<0xD2>': 213,\n",
       " '<0xD3>': 214,\n",
       " '<0xD4>': 215,\n",
       " '<0xD5>': 216,\n",
       " '<0xD6>': 217,\n",
       " '<0xD7>': 218,\n",
       " '<0xD8>': 219,\n",
       " '<0xD9>': 220,\n",
       " '<0xDA>': 221,\n",
       " '<0xDB>': 222,\n",
       " '<0xDC>': 223,\n",
       " '<0xDD>': 224,\n",
       " '<0xDE>': 225,\n",
       " '<0xDF>': 226,\n",
       " '<0xE0>': 227,\n",
       " '<0xE1>': 228,\n",
       " '<0xE2>': 229,\n",
       " '<0xE3>': 230,\n",
       " '<0xE4>': 231,\n",
       " '<0xE5>': 232,\n",
       " '<0xE6>': 233,\n",
       " '<0xE7>': 234,\n",
       " '<0xE8>': 235,\n",
       " '<0xE9>': 236,\n",
       " '<0xEA>': 237,\n",
       " '<0xEB>': 238,\n",
       " '<0xEC>': 239,\n",
       " '<0xED>': 240,\n",
       " '<0xEE>': 241,\n",
       " '<0xEF>': 242,\n",
       " '<0xF0>': 243,\n",
       " '<0xF1>': 244,\n",
       " '<0xF2>': 245,\n",
       " '<0xF3>': 246,\n",
       " '<0xF4>': 247,\n",
       " '<0xF5>': 248,\n",
       " '<0xF6>': 249,\n",
       " '<0xF7>': 250,\n",
       " '<0xF8>': 251,\n",
       " '<0xF9>': 252,\n",
       " '<0xFA>': 253,\n",
       " '<0xFB>': 254,\n",
       " '<0xFC>': 255,\n",
       " '<0xFD>': 256,\n",
       " '<0xFE>': 257,\n",
       " '<0xFF>': 258,\n",
       " '▁▁': 259,\n",
       " '▁t': 260,\n",
       " 'er': 261,\n",
       " 'in': 262,\n",
       " '▁a': 263,\n",
       " 'en': 264,\n",
       " 'on': 265,\n",
       " '▁th': 266,\n",
       " 'es': 267,\n",
       " '▁▁▁▁': 268,\n",
       " '▁s': 269,\n",
       " '▁d': 270,\n",
       " 'at': 271,\n",
       " 'or': 272,\n",
       " 'an': 273,\n",
       " '▁c': 274,\n",
       " 'is': 275,\n",
       " 're': 276,\n",
       " 'it': 277,\n",
       " '▁the': 278,\n",
       " 'ar': 279,\n",
       " 'le': 280,\n",
       " '▁w': 281,\n",
       " '▁p': 282,\n",
       " 'ou': 283,\n",
       " 'al': 284,\n",
       " '▁f': 285,\n",
       " '▁m': 286,\n",
       " 'ed': 287,\n",
       " '▁o': 288,\n",
       " '▁b': 289,\n",
       " 'om': 290,\n",
       " 'ion': 291,\n",
       " 'ing': 292,\n",
       " 'ic': 293,\n",
       " 'as': 294,\n",
       " 'el': 295,\n",
       " 'ent': 296,\n",
       " '▁in': 297,\n",
       " '▁h': 298,\n",
       " 'nd': 299,\n",
       " 'et': 300,\n",
       " '▁l': 301,\n",
       " '▁n': 302,\n",
       " 'st': 303,\n",
       " '▁to': 304,\n",
       " 'ch': 305,\n",
       " '▁I': 306,\n",
       " 'ro': 307,\n",
       " '▁▁▁▁▁▁▁▁': 308,\n",
       " 'il': 309,\n",
       " '▁of': 310,\n",
       " 'de': 311,\n",
       " 'ct': 312,\n",
       " '▁(': 313,\n",
       " 'am': 314,\n",
       " '▁C': 315,\n",
       " '▁de': 316,\n",
       " '▁S': 317,\n",
       " '▁u': 318,\n",
       " '▁A': 319,\n",
       " '▁\\\\': 320,\n",
       " '▁e': 321,\n",
       " '▁and': 322,\n",
       " '▁T': 323,\n",
       " 'ol': 324,\n",
       " '▁v': 325,\n",
       " 'im': 326,\n",
       " 'ot': 327,\n",
       " 'ad': 328,\n",
       " 'ut': 329,\n",
       " '▁g': 330,\n",
       " 'em': 331,\n",
       " 'ur': 332,\n",
       " 'id': 333,\n",
       " '▁*': 334,\n",
       " 'ig': 335,\n",
       " 'ra': 336,\n",
       " '▁re': 337,\n",
       " '▁is': 338,\n",
       " 'qu': 339,\n",
       " 'ow': 340,\n",
       " '▁M': 341,\n",
       " 'est': 342,\n",
       " '▁y': 343,\n",
       " 'se': 344,\n",
       " 've': 345,\n",
       " 'ce': 346,\n",
       " 'ie': 347,\n",
       " 'un': 348,\n",
       " '▁P': 349,\n",
       " '▁B': 350,\n",
       " 'ag': 351,\n",
       " 'ul': 352,\n",
       " '▁=': 353,\n",
       " 'he': 354,\n",
       " 'end': 355,\n",
       " 'ode': 356,\n",
       " 'ter': 357,\n",
       " 'ment': 358,\n",
       " 'os': 359,\n",
       " '▁D': 360,\n",
       " 'if': 361,\n",
       " 'ation': 362,\n",
       " '▁for': 363,\n",
       " '▁r': 364,\n",
       " '▁L': 365,\n",
       " '▁you': 366,\n",
       " '▁be': 367,\n",
       " 'ly': 368,\n",
       " 'ver': 369,\n",
       " 'ab': 370,\n",
       " 'te': 371,\n",
       " '▁it': 372,\n",
       " '▁on': 373,\n",
       " 'ri': 374,\n",
       " 'us': 375,\n",
       " '▁\"': 376,\n",
       " '▁wh': 377,\n",
       " '▁con': 378,\n",
       " '▁H': 379,\n",
       " '▁st': 380,\n",
       " 'ir': 381,\n",
       " '▁E': 382,\n",
       " '▁F': 383,\n",
       " 'ck': 384,\n",
       " '▁an': 385,\n",
       " 'th': 386,\n",
       " 'eg': 387,\n",
       " 'ay': 388,\n",
       " 'ith': 389,\n",
       " '▁R': 390,\n",
       " 'ist': 391,\n",
       " 'and': 392,\n",
       " '▁that': 393,\n",
       " '▁al': 394,\n",
       " '▁$': 395,\n",
       " '▁#': 396,\n",
       " 'od': 397,\n",
       " 'um': 398,\n",
       " '▁W': 399,\n",
       " 'ht': 400,\n",
       " 'code': 401,\n",
       " '▁G': 402,\n",
       " 'ate': 403,\n",
       " 'ess': 404,\n",
       " '▁N': 405,\n",
       " 'ere': 406,\n",
       " 'pp': 407,\n",
       " '▁as': 408,\n",
       " '▁se': 409,\n",
       " '▁pro': 410,\n",
       " '▁with': 411,\n",
       " 'pe': 412,\n",
       " '▁k': 413,\n",
       " 'ers': 414,\n",
       " 'pt': 415,\n",
       " ');': 416,\n",
       " 'lo': 417,\n",
       " '▁▁▁▁▁': 418,\n",
       " '▁com': 419,\n",
       " 'ame': 420,\n",
       " '▁`': 421,\n",
       " '▁Com': 422,\n",
       " 'ia': 423,\n",
       " 'ant': 424,\n",
       " '▁la': 425,\n",
       " '▁{': 426,\n",
       " '▁en': 427,\n",
       " 'ction': 428,\n",
       " '▁ex': 429,\n",
       " 'ld': 430,\n",
       " 'ub': 431,\n",
       " '▁j': 432,\n",
       " 'la': 433,\n",
       " 'ue': 434,\n",
       " '▁J': 435,\n",
       " 'ich': 436,\n",
       " '▁do': 437,\n",
       " '▁O': 438,\n",
       " '▁qu': 439,\n",
       " 'iv': 440,\n",
       " 'ort': 441,\n",
       " 'art': 442,\n",
       " '▁un': 443,\n",
       " '▁##': 444,\n",
       " '▁this': 445,\n",
       " 'ke': 446,\n",
       " '▁ha': 447,\n",
       " '▁-': 448,\n",
       " 'out': 449,\n",
       " '▁The': 450,\n",
       " '▁not': 451,\n",
       " '▁ne': 452,\n",
       " 'ill': 453,\n",
       " '▁le': 454,\n",
       " 'ci': 455,\n",
       " 'rom': 456,\n",
       " 'ine': 457,\n",
       " '//': 458,\n",
       " 'op': 459,\n",
       " 'egin': 460,\n",
       " '▁Comment': 461,\n",
       " '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁': 462,\n",
       " 'begin': 463,\n",
       " 'ст': 464,\n",
       " 'ass': 465,\n",
       " 'iz': 466,\n",
       " ').': 467,\n",
       " 'og': 468,\n",
       " '▁п': 469,\n",
       " '▁or': 470,\n",
       " '▁was': 471,\n",
       " '▁at': 472,\n",
       " 'our': 473,\n",
       " '▁i': 474,\n",
       " 'ain': 475,\n",
       " '▁K': 476,\n",
       " 'на': 477,\n",
       " '▁V': 478,\n",
       " 'ge': 479,\n",
       " '▁su': 480,\n",
       " 'ap': 481,\n",
       " 'age': 482,\n",
       " 'ould': 483,\n",
       " 'ne': 484,\n",
       " 'av': 485,\n",
       " 'xt': 486,\n",
       " 'ore': 487,\n",
       " 'ile': 488,\n",
       " '--': 489,\n",
       " '▁в': 490,\n",
       " '▁by': 491,\n",
       " 'li': 492,\n",
       " 'ath': 493,\n",
       " 'ра': 494,\n",
       " 'ber': 495,\n",
       " 'ach': 496,\n",
       " 'all': 497,\n",
       " '▁Th': 498,\n",
       " 'ult': 499,\n",
       " '▁}': 500,\n",
       " '▁U': 501,\n",
       " '▁us': 502,\n",
       " '▁z': 503,\n",
       " 'ust': 504,\n",
       " '▁have': 505,\n",
       " 'lic': 506,\n",
       " 'ни': 507,\n",
       " '▁can': 508,\n",
       " 'tr': 509,\n",
       " 'com': 510,\n",
       " '),': 511,\n",
       " '▁In': 512,\n",
       " 'ind': 513,\n",
       " 'ell': 514,\n",
       " '▁from': 515,\n",
       " 'ов': 516,\n",
       " 'to': 517,\n",
       " '▁[': 518,\n",
       " 'able': 519,\n",
       " 'ost': 520,\n",
       " '▁ch': 521,\n",
       " 'ect': 522,\n",
       " 'ight': 523,\n",
       " 'int': 524,\n",
       " \"▁'\": 525,\n",
       " '▁are': 526,\n",
       " '▁im': 527,\n",
       " '▁sh': 528,\n",
       " '▁<': 529,\n",
       " '▁An': 530,\n",
       " '▁с': 531,\n",
       " 'ata': 532,\n",
       " 'ire': 533,\n",
       " '▁tr': 534,\n",
       " 'con': 535,\n",
       " 'ord': 536,\n",
       " 'ity': 537,\n",
       " 'ard': 538,\n",
       " '▁▁▁▁▁▁': 539,\n",
       " '▁he': 540,\n",
       " '▁but': 541,\n",
       " 'oc': 542,\n",
       " '=\"': 543,\n",
       " '▁pr': 544,\n",
       " 'ure': 545,\n",
       " 'per': 546,\n",
       " 'ack': 547,\n",
       " 'ork': 548,\n",
       " 'ong': 549,\n",
       " 'ans': 550,\n",
       " 'ко': 551,\n",
       " 'ple': 552,\n",
       " '▁des': 553,\n",
       " 'ok': 554,\n",
       " 'orm': 555,\n",
       " 'wer': 556,\n",
       " 'ak': 557,\n",
       " 'pr': 558,\n",
       " 'ase': 559,\n",
       " '▁el': 560,\n",
       " 'ph': 561,\n",
       " 'ac': 562,\n",
       " '▁und': 563,\n",
       " '▁ar': 564,\n",
       " '▁if': 565,\n",
       " 'ud': 566,\n",
       " 'ps': 567,\n",
       " 'ite': 568,\n",
       " 'ble': 569,\n",
       " 'но': 570,\n",
       " 'fer': 571,\n",
       " 'pl': 572,\n",
       " 'ive': 573,\n",
       " 'ang': 574,\n",
       " 'ens': 575,\n",
       " 'ро': 576,\n",
       " '▁so': 577,\n",
       " 'so': 578,\n",
       " 'ast': 579,\n",
       " '()': 580,\n",
       " 'swer': 581,\n",
       " 'ru': 582,\n",
       " 'ies': 583,\n",
       " '▁:': 584,\n",
       " 'au': 585,\n",
       " 'ov': 586,\n",
       " 'ре': 587,\n",
       " 'го': 588,\n",
       " '▁der': 589,\n",
       " '▁my': 590,\n",
       " '▁we': 591,\n",
       " '▁me': 592,\n",
       " 'nt': 593,\n",
       " '▁ad': 594,\n",
       " 'urn': 595,\n",
       " '▁your': 596,\n",
       " '://': 597,\n",
       " 'are': 598,\n",
       " '▁all': 599,\n",
       " 'ff': 600,\n",
       " 'io': 601,\n",
       " 'estion': 602,\n",
       " 'ime': 603,\n",
       " '▁er': 604,\n",
       " 'lass': 605,\n",
       " '▁и': 606,\n",
       " '▁which': 607,\n",
       " 'ome': 608,\n",
       " 'ont': 609,\n",
       " '▁par': 610,\n",
       " '▁ma': 611,\n",
       " '▁Y': 612,\n",
       " '\",': 613,\n",
       " '▁о': 614,\n",
       " 'ft': 615,\n",
       " 'ial': 616,\n",
       " 'cc': 617,\n",
       " 'ound': 618,\n",
       " '▁li': 619,\n",
       " '▁res': 620,\n",
       " 'eth': 621,\n",
       " 'ject': 622,\n",
       " '▁app': 623,\n",
       " '▁St': 624,\n",
       " 'ice': 625,\n",
       " '▁am': 626,\n",
       " 'act': 627,\n",
       " '▁del': 628,\n",
       " 'gr': 629,\n",
       " 'ated': 630,\n",
       " 'ier': 631,\n",
       " '▁▁▁▁▁▁▁▁▁▁▁▁': 632,\n",
       " '▁ab': 633,\n",
       " '▁et': 634,\n",
       " 'ally': 635,\n",
       " '..': 636,\n",
       " 'port': 637,\n",
       " 'ik': 638,\n",
       " '▁per': 639,\n",
       " '▁cont': 640,\n",
       " 'ри': 641,\n",
       " 'ка': 642,\n",
       " 'ser': 643,\n",
       " 'ли': 644,\n",
       " 'll': 645,\n",
       " 'iew': 646,\n",
       " 'ign': 647,\n",
       " '_{': 648,\n",
       " 'put': 649,\n",
       " 'one': 650,\n",
       " 'unction': 651,\n",
       " '▁di': 652,\n",
       " 'ary': 653,\n",
       " 'ition': 654,\n",
       " 'ma': 655,\n",
       " 'ен': 656,\n",
       " 'get': 657,\n",
       " '▁lo': 658,\n",
       " '▁val': 659,\n",
       " '▁Q': 660,\n",
       " 'ran': 661,\n",
       " '▁д': 662,\n",
       " 'ence': 663,\n",
       " '▁work': 664,\n",
       " '▁на': 665,\n",
       " 'ip': 666,\n",
       " 'item': 667,\n",
       " 'ype': 668,\n",
       " '▁&': 669,\n",
       " '▁his': 670,\n",
       " '▁use': 671,\n",
       " 'der': 672,\n",
       " '▁Answer': 673,\n",
       " '▁will': 674,\n",
       " 'ize': 675,\n",
       " 'та': 676,\n",
       " 'low': 677,\n",
       " '▁Ch': 678,\n",
       " '▁get': 679,\n",
       " 'ide': 680,\n",
       " 'ous': 681,\n",
       " 'ink': 682,\n",
       " 'ption': 683,\n",
       " 'ла': 684,\n",
       " 'turn': 685,\n",
       " 'ung': 686,\n",
       " 'ec': 687,\n",
       " 'ug': 688,\n",
       " 'form': 689,\n",
       " 'res': 690,\n",
       " 'htt': 691,\n",
       " 'oug': 692,\n",
       " 'ль': 693,\n",
       " '▁no': 694,\n",
       " 'cl': 695,\n",
       " '▁ro': 696,\n",
       " '▁one': 697,\n",
       " 'tt': 698,\n",
       " 'cri': 699,\n",
       " 'du': 700,\n",
       " '▁up': 701,\n",
       " 'то': 702,\n",
       " '(\"': 703,\n",
       " '▁ob': 704,\n",
       " 'we': 705,\n",
       " 'ory': 706,\n",
       " '▁est': 707,\n",
       " 'ery': 708,\n",
       " 'iel': 709,\n",
       " 'str': 710,\n",
       " 'ob': 711,\n",
       " '▁que': 712,\n",
       " 'ian': 713,\n",
       " '▁out': 714,\n",
       " '▁pl': 715,\n",
       " '▁new': 716,\n",
       " 'ки': 717,\n",
       " '▁+': 718,\n",
       " 'ry': 719,\n",
       " 'oth': 720,\n",
       " 'ther': 721,\n",
       " '▁var': 722,\n",
       " '▁would': 723,\n",
       " '▁ser': 724,\n",
       " 'tern': 725,\n",
       " 'text': 726,\n",
       " '▁there': 727,\n",
       " 'ish': 728,\n",
       " 'ror': 729,\n",
       " 'те': 730,\n",
       " '▁set': 731,\n",
       " '▁@': 732,\n",
       " '▁по': 733,\n",
       " '▁te': 734,\n",
       " 'ex': 735,\n",
       " '▁return': 736,\n",
       " 'ail': 737,\n",
       " '▁any': 738,\n",
       " '▁It': 739,\n",
       " '▁function': 740,\n",
       " '{\\\\': 741,\n",
       " \"',\": 742,\n",
       " 'és': 743,\n",
       " 'ale': 744,\n",
       " 'ан': 745,\n",
       " '▁when': 746,\n",
       " 'ib': 747,\n",
       " '▁go': 748,\n",
       " 'ance': 749,\n",
       " '▁had': 750,\n",
       " '▁Qu': 751,\n",
       " '▁comp': 752,\n",
       " 'ле': 753,\n",
       " '▁з': 754,\n",
       " 'math': 755,\n",
       " '▁has': 756,\n",
       " '▁м': 757,\n",
       " '▁pre': 758,\n",
       " 'ener': 759,\n",
       " '▁part': 760,\n",
       " 'elf': 761,\n",
       " '▁die': 762,\n",
       " '▁like': 763,\n",
       " 'ray': 764,\n",
       " 'irst': 765,\n",
       " '▁dis': 766,\n",
       " '▁man': 767,\n",
       " 'rit': 768,\n",
       " '▁then': 769,\n",
       " '▁class': 770,\n",
       " 'pro': 771,\n",
       " '▁po': 772,\n",
       " '▁using': 773,\n",
       " 'eb': 774,\n",
       " '▁code': 775,\n",
       " 'own': 776,\n",
       " '▁some': 777,\n",
       " 'ces': 778,\n",
       " '▁$\\\\': 779,\n",
       " 'ер': 780,\n",
       " 'lect': 781,\n",
       " '▁au': 782,\n",
       " 'isch': 783,\n",
       " '▁col': 784,\n",
       " '▁–': 785,\n",
       " 'up': 786,\n",
       " 'ons': 787,\n",
       " '▁add': 788,\n",
       " 'ild': 789,\n",
       " 'iss': 790,\n",
       " 'val': 791,\n",
       " 'ount': 792,\n",
       " 'les': 793,\n",
       " 'vent': 794,\n",
       " '▁▁▁▁▁▁▁▁▁▁▁▁▁': 795,\n",
       " '▁Z': 796,\n",
       " 'In': 797,\n",
       " 'row': 798,\n",
       " 'ear': 799,\n",
       " 'ations': 800,\n",
       " 'ah': 801,\n",
       " 'que': 802,\n",
       " 'ublic': 803,\n",
       " 'ank': 804,\n",
       " '▁sp': 805,\n",
       " '▁Wh': 806,\n",
       " '----': 807,\n",
       " 'sk': 808,\n",
       " 'ew': 809,\n",
       " 'ags': 810,\n",
       " 'ти': 811,\n",
       " 'ann': 812,\n",
       " '▁—': 813,\n",
       " 'ert': 814,\n",
       " 'ace': 815,\n",
       " 'sch': 816,\n",
       " '▁need': 817,\n",
       " '▁à': 818,\n",
       " 'ien': 819,\n",
       " 'ough': 820,\n",
       " 'не': 821,\n",
       " '▁def': 822,\n",
       " 'ij': 823,\n",
       " 'ern': 824,\n",
       " '▁what': 825,\n",
       " '▁Ar': 826,\n",
       " 'wo': 827,\n",
       " 'ml': 828,\n",
       " '</': 829,\n",
       " '▁Re': 830,\n",
       " '▁es': 831,\n",
       " '▁inst': 832,\n",
       " 'bo': 833,\n",
       " 'az': 834,\n",
       " '▁###': 835,\n",
       " '▁б': 836,\n",
       " 'erm': 837,\n",
       " '▁Al': 838,\n",
       " 'led': 839,\n",
       " 'да': 840,\n",
       " 'ten': 841,\n",
       " 'set': 842,\n",
       " 'ло': 843,\n",
       " '▁comm': 844,\n",
       " 'sh': 845,\n",
       " 'ва': 846,\n",
       " '▁/': 847,\n",
       " '▁data': 848,\n",
       " '▁//': 849,\n",
       " '](': 850,\n",
       " '▁str': 851,\n",
       " 'ose': 852,\n",
       " '▁Un': 853,\n",
       " 'ven': 854,\n",
       " 'St': 855,\n",
       " '...': 856,\n",
       " '▁С': 857,\n",
       " 'yst': 858,\n",
       " '▁«': 859,\n",
       " 'ick': 860,\n",
       " 'ix': 861,\n",
       " 'par': 862,\n",
       " '▁у': 863,\n",
       " '▁want': 864,\n",
       " 'ng': 865,\n",
       " 'ote': 866,\n",
       " '▁gr': 867,\n",
       " '▁du': 868,\n",
       " '▁.': 869,\n",
       " 'und': 870,\n",
       " '▁only': 871,\n",
       " '▁sa': 872,\n",
       " 'ely': 873,\n",
       " 'vers': 874,\n",
       " '▁ent': 875,\n",
       " '))': 876,\n",
       " \"('\": 877,\n",
       " '▁mod': 878,\n",
       " 'ava': 879,\n",
       " 'ton': 880,\n",
       " '▁should': 881,\n",
       " 'ement': 882,\n",
       " '▁form': 883,\n",
       " '▁also': 884,\n",
       " '▁sc': 885,\n",
       " 'ings': 886,\n",
       " '▁You': 887,\n",
       " 'ón': 888,\n",
       " '▁kn': 889,\n",
       " '();': 890,\n",
       " '▁|': 891,\n",
       " '▁were': 892,\n",
       " 'ss': 893,\n",
       " '▁Question': 894,\n",
       " 'ise': 895,\n",
       " '▁they': 896,\n",
       " '▁De': 897,\n",
       " 'ond': 898,\n",
       " '▁sol': 899,\n",
       " '▁fol': 900,\n",
       " '▁more': 901,\n",
       " '▁her': 902,\n",
       " '▁_': 903,\n",
       " '▁é': 904,\n",
       " 'atch': 905,\n",
       " 'fter': 906,\n",
       " '▁cre': 907,\n",
       " 'lock': 908,\n",
       " 'tring': 909,\n",
       " '▁This': 910,\n",
       " 'ze': 911,\n",
       " 'ado': 912,\n",
       " 'ull': 913,\n",
       " 'ger': 914,\n",
       " 'be': 915,\n",
       " '▁other': 916,\n",
       " '▁Tags': 917,\n",
       " 'ution': 918,\n",
       " 'ict': 919,\n",
       " '▁how': 920,\n",
       " '▁x': 921,\n",
       " '▁Se': 922,\n",
       " '▁che': 923,\n",
       " 'cript': 924,\n",
       " '▁just': 925,\n",
       " '▁pos': 926,\n",
       " 'ange': 927,\n",
       " 'ific': 928,\n",
       " 'ree': 929,\n",
       " '}}': 930,\n",
       " '▁time': 931,\n",
       " 'app': 932,\n",
       " 'ны': 933,\n",
       " '▁file': 934,\n",
       " 'ark': 935,\n",
       " 'ical': 936,\n",
       " '▁first': 937,\n",
       " '▁int': 938,\n",
       " '▁В': 939,\n",
       " '▁He': 940,\n",
       " 'ta': 941,\n",
       " 'ument': 942,\n",
       " 'ors': 943,\n",
       " 'lement': 944,\n",
       " 'rac': 945,\n",
       " '▁ag': 946,\n",
       " '▁does': 947,\n",
       " 'yn': 948,\n",
       " 'read': 949,\n",
       " 'ual': 950,\n",
       " '▁Le': 951,\n",
       " 'ys': 952,\n",
       " '▁em': 953,\n",
       " '▁num': 954,\n",
       " 'vel': 955,\n",
       " 'ди': 956,\n",
       " 'over': 957,\n",
       " '▁dif': 958,\n",
       " 'ethod': 959,\n",
       " '▁If': 960,\n",
       " '▁spe': 961,\n",
       " 'ym': 962,\n",
       " '▁them': 963,\n",
       " '▁into': 964,\n",
       " '▁▁▁▁▁▁▁▁▁▁': 965,\n",
       " '▁les': 966,\n",
       " '▁its': 967,\n",
       " 'ese': 968,\n",
       " 'ield': 969,\n",
       " '▁public': 970,\n",
       " '▁П': 971,\n",
       " '▁den': 972,\n",
       " 'ystem': 973,\n",
       " 'of': 974,\n",
       " '▁over': 975,\n",
       " '->': 976,\n",
       " '▁fil': 977,\n",
       " 'name': 978,\n",
       " 'inal': 979,\n",
       " '▁il': 980,\n",
       " 'ample': 981,\n",
       " '▁way': 982,\n",
       " 'ica': 983,\n",
       " 'во': 984,\n",
       " 'cess': 985,\n",
       " 'itt': 986,\n",
       " 'uch': 987,\n",
       " '▁where': 988,\n",
       " 'ми': 989,\n",
       " 'org': 990,\n",
       " 'https': 991,\n",
       " '▁vo': 992,\n",
       " 'ient': 993,\n",
       " 'ove': 994,\n",
       " '▁value': 995,\n",
       " 'eng': 996,\n",
       " '▁La': 997,\n",
       " '^{': 998,\n",
       " 'ref': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Special_tokens = ['<bos>', 'EOS', '<unk>', 'SEP', 'PAD', 'CLS', 'MASK'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPECIAL_TOKENS_ATTRIBUTES', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_tokens', '_added_tokens_decoder', '_added_tokens_encoder', '_additional_special_tokens', '_auto_class', '_batch_encode_plus', '_batch_prepare_for_model', '_bos_token', '_call_one', '_cls_token', '_compile_jinja_template', '_convert_id_to_token', '_convert_token_to_id', '_convert_token_to_id_with_added_voc', '_create_repo', '_decode', '_decode_use_source_tokenizer', '_encode_plus', '_eos_token', '_eventual_warn_about_too_long_sequence', '_eventually_correct_t5_max_length', '_from_pretrained', '_get_files_timestamps', '_get_padding_truncation_strategies', '_in_target_context_manager', '_mask_token', '_pad', '_pad_token', '_pad_token_type_id', '_processor_class', '_save_pretrained', '_sep_token', '_set_processor_class', '_switch_to_input_mode', '_switch_to_target_mode', '_tokenize', '_unk_token', '_update_trie', '_upload_modified_files', 'add_bos_token', 'add_eos_token', 'add_special_tokens', 'add_tokens', 'added_tokens_decoder', 'added_tokens_encoder', 'additional_special_tokens', 'additional_special_tokens_ids', 'all_special_ids', 'all_special_tokens', 'all_special_tokens_extended', 'apply_chat_template', 'as_target_tokenizer', 'batch_decode', 'batch_encode_plus', 'bos_token', 'bos_token_id', 'build_inputs_with_special_tokens', 'chat_template', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id', 'convert_added_tokens', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'default_chat_template', 'deprecation_warnings', 'encode', 'encode_plus', 'eos_token', 'eos_token_id', 'from_pretrained', 'get_added_vocab', 'get_special_tokens_mask', 'get_spm_processor', 'get_vocab', 'init_inputs', 'init_kwargs', 'is_fast', 'legacy', 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'max_model_input_sizes', 'model_input_names', 'model_max_length', 'name_or_path', 'num_special_tokens_to_add', 'pad', 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_for_tokenization', 'prepare_seq2seq_batch', 'pretrained_init_configuration', 'pretrained_vocab_files_map', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'sep_token', 'sep_token_id', 'slow_tokenizer_class', 'sp_model', 'sp_model_kwargs', 'special_tokens_map', 'special_tokens_map_extended', 'split_special_tokens', 'tokenize', 'tokens_trie', 'truncate_sequences', 'truncation_side', 'unk_token', 'unk_token_id', 'unk_token_length', 'use_default_system_prompt', 'verbose', 'vocab_file', 'vocab_files_names', 'vocab_size']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 1\n",
      "</s> 2\n",
      "None None\n",
      "<unk> 0\n",
      "None None\n",
      "None None\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.mask_token, tokenizer.mask_token_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. tokenizer.json 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_json_path)\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(tokenizer_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_compile_jinja_template',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'default_chat_template',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fast_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(fast_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fast_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "builtin_function_or_method"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fast_tokenizer.vocab.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fast_tokenizer.vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = fast_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수 30970\n",
      "무 31716\n",
      "소 31189\n",
      "연 31285\n",
      "만 31826\n",
      "교 31972\n",
      "드 31493\n",
      "민 31582\n",
      "차 31817\n",
      "전 31170\n",
      "재 31973\n",
      "에 31054\n",
      "정 30852\n",
      "원 31198\n",
      "안 31734\n",
      "식 31895\n",
      "구 31231\n",
      "면 31747\n",
      "해 31435\n",
      "로 30906\n",
      "문 31406\n",
      "경 31378\n",
      "한 30877\n",
      "오 31346\n",
      "화 31225\n",
      "미 31362\n",
      "일 31153\n",
      "요 31527\n",
      "회 31953\n",
      "진 31536\n",
      "자 31013\n",
      "인 30918\n",
      "과 31906\n",
      "위 31724\n",
      "라 31197\n",
      "강 31774\n",
      "이 30393\n",
      "을 31286\n",
      "씨 31781\n",
      "호 31603\n",
      "유 31533\n",
      "개 31789\n",
      "천 31563\n",
      "니 31063\n",
      "비 31487\n",
      "사 30791\n",
      "년 31571\n",
      "보 31199\n",
      "산 31458\n",
      "합 31980\n",
      "용 31737\n",
      "명 31976\n",
      "선 31345\n",
      "박 31736\n",
      "방 31945\n",
      "현 31680\n",
      "다 30709\n",
      "김 31102\n",
      "왕 31996\n",
      "상 31158\n",
      "모 31962\n",
      "지 30811\n",
      "터 31856\n",
      "시 30889\n",
      "서 31093\n",
      "남 31754\n",
      "세 31578\n",
      "제 31306\n",
      "동 31000\n",
      "는 31081\n",
      "도 31136\n",
      "우 31327\n",
      "들 31804\n",
      "조 31408\n",
      "어 31129\n",
      "단 31746\n",
      "역 31987\n",
      "스 30784\n",
      "를 31517\n",
      "군 31699\n",
      "신 31262\n",
      "월 31950\n",
      "국 31293\n",
      "아 30860\n",
      "타 31925\n",
      "여 31457\n",
      "부 31279\n",
      "마 31417\n",
      "성 31126\n",
      "리 30826\n",
      "기 30827\n",
      "의 30708\n",
      "대 30890\n",
      "음 31966\n",
      "주 30981\n",
      "가 30903\n",
      "은 31354\n",
      "내 31940\n",
      "나 31207\n",
      "바 31963\n",
      "하 30944\n",
      "학 31822\n",
      "트 31177\n",
      "그 31607\n",
      "종 31930\n",
      "백 31989\n",
      "영 31288\n",
      "장 31299\n",
      "고 31137\n",
      "공 31334\n",
      "중 31941\n"
     ]
    }
   ],
   "source": [
    "korean_reg = re.compile(r'[ㄱ-ㅣ가-힣]')\n",
    "korean_dict = {}\n",
    "for key, value in vocab_dict.items():\n",
    "    if korean_reg.match(key):\n",
    "        print(key, value)\n",
    "        korean_dict[key] = value\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(korean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['수', '무', '소', '연', '만', '교', '드', '민', '차', '전', '재', '에', '정', '원', '안', '식', '구', '면', '해', '로', '문', '경', '한', '오', '화', '미', '일', '요', '회', '진', '자', '인', '과', '위', '라', '강', '이', '을', '씨', '호', '유', '개', '천', '니', '비', '사', '년', '보', '산', '합', '용', '명', '선', '박', '방', '현', '다', '김', '왕', '상', '모', '지', '터', '시', '서', '남', '세', '제', '동', '는', '도', '우', '들', '조', '어', '단', '역', '스', '를', '군', '신', '월', '국', '아', '타', '여', '부', '마', '성', '리', '기', '의', '대', '음', '주', '가', '은', '내', '나', '바', '하', '학', '트', '그', '종', '백', '영', '장', '고', '공', '중'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31822"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['학']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['osci',\n",
       " 'ammer',\n",
       " 'прави',\n",
       " '▁happening',\n",
       " 'Ă',\n",
       " '▁specifying',\n",
       " 'itzerland',\n",
       " '▁crow',\n",
       " 'inas',\n",
       " '{.',\n",
       " '▁Националь',\n",
       " '▁select',\n",
       " 'bounds',\n",
       " '▁cuer',\n",
       " 'тан',\n",
       " 'sdl',\n",
       " '龙',\n",
       " 'adding',\n",
       " '▁Dav',\n",
       " '▁lavor',\n",
       " 'htm',\n",
       " '▁tons',\n",
       " '▁Fr',\n",
       " \"`'\",\n",
       " 'ject',\n",
       " '▁começ',\n",
       " '면',\n",
       " 'preventDefault',\n",
       " 'undefined',\n",
       " '▁fu',\n",
       " 'lywood',\n",
       " 'た',\n",
       " '▁dével',\n",
       " 'дова',\n",
       " 'yan',\n",
       " '▁once',\n",
       " '▁pak',\n",
       " 'isk',\n",
       " '▁gender',\n",
       " 'ப',\n",
       " '▁intellig',\n",
       " 'Đ',\n",
       " 'дав',\n",
       " 'tis',\n",
       " 'сона',\n",
       " '▁atmos',\n",
       " '}},',\n",
       " 'erei',\n",
       " 'inflate',\n",
       " 'dw',\n",
       " '▁-->',\n",
       " '▁Philadelphia',\n",
       " 'NET',\n",
       " '▁answering',\n",
       " 'giv',\n",
       " 'rewrite',\n",
       " 'ALSE',\n",
       " '▁usage',\n",
       " 'gas',\n",
       " '▁p',\n",
       " 'REE',\n",
       " '▁curiosity',\n",
       " 'idos',\n",
       " 'acing',\n",
       " 'IMAGE',\n",
       " '▁Lot',\n",
       " '▁Deutsch',\n",
       " 'MODE',\n",
       " 'href',\n",
       " 'elijke',\n",
       " '值',\n",
       " 'nak',\n",
       " 'Why',\n",
       " 'Selection',\n",
       " 'PAR',\n",
       " 'amar',\n",
       " '▁Georgia',\n",
       " '<0x13>',\n",
       " '▁Guardian',\n",
       " '▁free',\n",
       " 'labels',\n",
       " 'Len',\n",
       " '▁outras',\n",
       " '▁relate',\n",
       " '▁Ig',\n",
       " 'kon',\n",
       " 'reate',\n",
       " 'fahr',\n",
       " 'ædia',\n",
       " 'alf',\n",
       " 'asha',\n",
       " 'ach',\n",
       " '▁foreach',\n",
       " '▁Sand',\n",
       " 'Bind',\n",
       " '<0x4E>',\n",
       " '▁І',\n",
       " '▁Publish',\n",
       " 'FileName',\n",
       " 'дні',\n",
       " 'пер',\n",
       " '▁conj',\n",
       " '▁looks',\n",
       " 'bourne',\n",
       " '▁march',\n",
       " '▁satisfied',\n",
       " '明',\n",
       " '▁Sy',\n",
       " 'Variable',\n",
       " 'umen',\n",
       " '▁particulier',\n",
       " '▁chairman',\n",
       " 'limp',\n",
       " 'ся',\n",
       " 'alph',\n",
       " '▁получил',\n",
       " '▁fait',\n",
       " 'oso',\n",
       " '▁fixing',\n",
       " '▁fashion']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_dict.keys())[:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BertWordPieceTokenizer : BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)\n",
    "- CharBPETokenizer : 오리지널 BPE\n",
    "- ByteLevelBPETokenizer : BPE의 바이트 레벨 버전\n",
    "- SentencePieceBPETokenizer : 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama-2-ko-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../llama_models/llama-2-ko-7b'\n",
    "json_path = os.path.join(root, 'tokenizer.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46336\n"
     ]
    }
   ],
   "source": [
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=json_path)\n",
    "\n",
    "print(len(fast_tokenizer.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPECIAL_TOKENS_ATTRIBUTES', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_tokens', '_additional_special_tokens', '_auto_class', '_batch_encode_plus', '_bos_token', '_call_one', '_cls_token', '_compile_jinja_template', '_convert_encoding', '_convert_id_to_token', '_convert_token_to_id_with_added_voc', '_create_repo', '_decode', '_decode_use_source_tokenizer', '_encode_plus', '_eos_token', '_eventual_warn_about_too_long_sequence', '_eventually_correct_t5_max_length', '_from_pretrained', '_get_files_timestamps', '_get_padding_truncation_strategies', '_in_target_context_manager', '_mask_token', '_pad', '_pad_token', '_pad_token_type_id', '_processor_class', '_save_pretrained', '_sep_token', '_set_processor_class', '_switch_to_input_mode', '_switch_to_target_mode', '_tokenizer', '_unk_token', '_upload_modified_files', 'add_special_tokens', 'add_tokens', 'added_tokens_decoder', 'added_tokens_encoder', 'additional_special_tokens', 'additional_special_tokens_ids', 'all_special_ids', 'all_special_tokens', 'all_special_tokens_extended', 'apply_chat_template', 'as_target_tokenizer', 'backend_tokenizer', 'batch_decode', 'batch_encode_plus', 'bos_token', 'bos_token_id', 'build_inputs_with_special_tokens', 'can_save_slow_tokenizer', 'chat_template', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id', 'convert_added_tokens', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'decoder', 'default_chat_template', 'deprecation_warnings', 'encode', 'encode_plus', 'eos_token', 'eos_token_id', 'from_pretrained', 'get_added_vocab', 'get_special_tokens_mask', 'get_vocab', 'init_inputs', 'init_kwargs', 'is_fast', 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'max_model_input_sizes', 'model_input_names', 'model_max_length', 'name_or_path', 'num_special_tokens_to_add', 'pad', 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_seq2seq_batch', 'pretrained_init_configuration', 'pretrained_vocab_files_map', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'sep_token', 'sep_token_id', 'set_truncation_and_padding', 'slow_tokenizer_class', 'special_tokens_map', 'special_tokens_map_extended', 'split_special_tokens', 'tokenize', 'train_new_from_iterator', 'truncate_sequences', 'truncation_side', 'unk_token', 'unk_token_id', 'verbose', 'vocab', 'vocab_files_names', 'vocab_size']\n"
     ]
    }
   ],
   "source": [
    "print(dir(fast_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "None None\n",
      "None None\n",
      "None None\n",
      "None None\n",
      "None None\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(fast_tokenizer.bos_token, fast_tokenizer.bos_token_id)\n",
    "print(fast_tokenizer.eos_token, fast_tokenizer.eos_token_id)\n",
    "print(fast_tokenizer.pad_token, fast_tokenizer.pad_token_id)\n",
    "print(fast_tokenizer.unk_token, fast_tokenizer.unk_token_id)\n",
    "print(fast_tokenizer.sep_token, fast_tokenizer.sep_token_id)\n",
    "print(fast_tokenizer.cls_token, fast_tokenizer.cls_token_id)\n",
    "print(fast_tokenizer.mask_token, fast_tokenizer.mask_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_reg2 = re.compile(r'[ㄱ-ㅣ가-힣]')\n",
    "\n",
    "korean_dict2 = {}\n",
    "\n",
    "for key, value in fast_tokenizer.vocab.items():\n",
    "    if korean_reg2.match(key):\n",
    "        korean_dict2[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['지긋', '싸고', '니까', '가야', '받지', '롯데', '팀', '불어', '간담', '틸', '총', '둔', '전쟁', '대지', '시장에', '주를', '과제', '잔아', '추진', '이노', '활', '랫폼', '났네', '고에', '보며', '다는게', '려요', '육', '정이', '주냐', '폐', '흙', '싶다', '슷', '뺑', '해야지', '네요', '룬', '는구나', '흥민', '지막', '입', '췌', '였네', '려라', '국민을', '통에', '하기는', '본부는', '적', '햄', '왜구', '뎅', '하시는', '금으로', '거야', '겠네', '이에', '유산', '갠', '내면', '것처럼', '아아', '트를', '렸습니다', '으니', '감으로', '회사', '호텔', '꼬', '핏', '핌', '경은', '국과', '악', '지를', '형을', '박물', '심에서', '상납', '안전', '챗', '깥', '세가', '처분', '롯', '절', '제도', '되니', '누구', '인민', '들로', '마음에', '려니', '뀔', '할것', '트롤', '입국', '진보', '습']\n"
     ]
    }
   ],
   "source": [
    "print(list(korean_dict2.keys())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5527\n"
     ]
    }
   ],
   "source": [
    "print(len(korean_dict2.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('llama_recipes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "370669f31d56c793d418c7345ca8413592cca298019e995be7fff47287a2522c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
